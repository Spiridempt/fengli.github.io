This folder contains the data and scripts we have used in paper "How Does Bug-Handling Effort Differ Among Different Programming Languages?".

The scripts are in the "scripts" folder, and the data are in the "data-for-analysis" folder. The following explanations are for the latter folder.

As mentioned in the paper, all source data are collected through GitHub API. Firstly, we will introduce some of our source data.

The folder "contributors" contains the information of contributors in every project perspectively. Each project has a file called "username+'-'+repositoryname+'.csv'". In each file, the four columns represent "UserName", "UserID", "UserType", "contributions", respectively.

The folder "CLOC" contains the SLOC information of every project perspectively. Each language has a file call "'cloc+'+languagename+'.csv'". In each file, each row represents a project, and the information contains "repository name" and "SLOC" collected by CLOC.

In the folder "projects_list", there are 10 files named "language+'.txt'". These are projects we used in this work.

In the root directory, there is a file called "issue_count.csv", which records the number of issues of each project.

In the root directory, there is a file called "duration.csv", which records the duration of each project.

The origin data (commits information) collected from GitHub are up to 100G, which is too large to provide. So, we only put the intermediate results here, which can be done by preprocessing the origin data. As we can see, in folder "Commit", there are 10 folders named by 10 languages. In each folder, each file contains the commit information of one project. Inside one file, a row represents a commit, and the columns represent "commitSHA", "parentcommitSHA", "total change lines", "additions", "deletions", "commit type", "Issue or not", "Bug or not", "changes of lines of file in this language", "Refactor or not", "file change number", respectively.

Now, we move to the script "get_commit_information.py". This script gets the information for our analysis. After runnning it, we will get 10 files located in folder "results". We directly put these data here. The meaning of each column is list in the first row.

The last origin data (issues information) collected from GitHub are up to 50G, which is too large to provide. So, we only put the intermediate results here, which can be done by preprocessing the origin data. In the folder 'Issue', there are 10 sub-folders named by language. In each folder, each file represents one project. Each row represents an bug issue, and the meanings of columns are "issueID", "opentime", "closetime", and time of every comment then.

The script "get_bug_handling_time.py" can calculate the median of bug-handling time from above data. The output file "time.csv" contains each project as a row. The meanings of columns are "username", "repositoryname", "language", "median handling time", "median comment num", respectively.

The script "combine_commit_time.py" combine the data from commits information and issues information, and output a csv file which contains there information (result_interdediate.csv).

The script "get_other_information.py" add "refactor_count", "filechange_count", issue_count", "duration", "contributors", "agemonth", "ageyear" to the csv file above, and output a new csv file (result_final.csv).



Put it together, to process our data, you should enter these commands (Python3 environment):
Step1: python get_commit_information.py
Step2: python get_bug_handling_time.py
Step3: python combine_commit_time.py
Step4: python get_other_information.py
